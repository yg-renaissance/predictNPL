{"cells":[{"metadata":{"trusted":true,"_uuid":"d67a817043afb7153c053a561d29ae0bb37cd578"},"cell_type":"code","source":"# I will use in this Kernel the step-by-step process of Will Koehrsen.\n# I won't use everything, but most of them.\n# This project at in GitHub repository: https://github.com/WillKoehrsen/machine-learning-project-walkthrough","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# # Imports\n\n# Pandas and numpy for data manipulation\nimport pandas as pd\nimport numpy as np\n\n# No warnings about setting value on copy of slice\npd.options.mode.chained_assignment = None\n\n# Display up to 60 columns of a dataframe\npd.set_option('display.max_columns', 60)\n\n# Matplotlib visualization\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Set default font size\nplt.rcParams['font.size'] = 24\n\n# Internal ipython tool for setting figure size\nfrom IPython.core.pylabtools import figsize\n\n# Seaborn for visualization\nimport seaborn as sns\nsns.set(font_scale = 2)\n\n# Splitting data into training and testing\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# # # Data Cleaning and Formatting\n\n# # Load in the Data and Examine\n\n# Read in data into a dataframe \ndata = pd.read_csv('../input/train_v2.csv')\n\n# Display top of dataframe\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59f931574dd75b9b1b750403420b04c3216cef56"},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0afdcffbcd53eb30140dc2c8bcd922f9b7cef9d2"},"cell_type":"code","source":"# # Data Types and Missing Values\n\n# See the column data types and non-missing values\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e66dd9afd8d014c48077843b26f304a9c8e8c130"},"cell_type":"code","source":"data.select_dtypes(include=['object']).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e62f799ae387d53f80a3261f6ec2ead90cef0a4"},"cell_type":"code","source":"# Statistics for each column\ndata.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"031a49103636eaa5dc31f4b6c2245aed1dfd19f1"},"cell_type":"code","source":"# # Missing Values\n\n# Function to calculate missing values by column\ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() / len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b9e205e77fa49de21972eb4884477ac0f8bc349","scrolled":true},"cell_type":"code","source":"missing_values_table(data).head(50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c14e564d273adf7a71072ca75e1906e8e3d3b0a6"},"cell_type":"code","source":"data.fillna(data.mean(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee7b2b30918685d90ad4b3062e205c7c81172ee3"},"cell_type":"code","source":"missing_values_table(data).head(50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a9d4567ec2eaa47b63110c90df17d6e38aeee87"},"cell_type":"code","source":"data.dropna(inplace=True)\nmissing_values_table(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2db9645d013be1732179ef0f682a2268c24df15d"},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d04cdc680bc805aeabaf6afcbc7d1f6c03e0e7d3"},"cell_type":"code","source":"# # # Exploratory Data Analysis\n\nfor i in data.select_dtypes(include=['object']).columns:\n    data.drop(labels=i, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90cc9e82a25bd463d7a6c477cfdc8e4680eaf1b8"},"cell_type":"code","source":"# # Single Variable Plots\n\nfigsize=(8, 8)\n\n# Histogram of the loss\nplt.style.use('fivethirtyeight')\nplt.hist(data['loss'], bins = 100, edgecolor = 'k')\nplt.xlabel('Loss') \nplt.ylabel('Number of Clients');\nplt.title('Loss Distribution')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1fe9f16cf8ed034ee3bb0e6fbd66f76f071746c2"},"cell_type":"code","source":"# # Correlations between Features and Target\n\n# Find all correlations and sort \ncorrelations_data = data.corr()['loss'].sort_values()\n\n# Print the most negative correlations\nprint(correlations_data.head(15), '\\n')\n\n# Print the most positive correlations\nprint(correlations_data.tail(15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"61a89b14640fbc08f925bb29e6fa17ec9cc757af","scrolled":false},"cell_type":"code","source":"for i in data.columns:\n    if len(set(data[i]))==1:\n        data.drop(labels=[i], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e04f8eadeeab0cdfffa98923e49f95b4f89064c"},"cell_type":"code","source":"# Find all correlations and sort \ncorrelations_data = data.corr()['loss'].sort_values()\n\n# Print the most negative correlations\nprint(correlations_data.head(15), '\\n')\n\n# Print the most positive correlations\nprint(correlations_data.tail(15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"668d758b12480015588a2dbf8a09268ac98adf08"},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"19e87448a6d0ba70d09f1ceb9901b8b4cbdd07c8"},"cell_type":"code","source":"# # # Feature Engineering and Selection\n\ndef remove_collinear_features(x, threshold):\n    '''\n    Objective:\n        Remove collinear features in a dataframe with a correlation coefficient\n        greater than the threshold. Removing collinear features can help a model\n        to generalize and improves the interpretability of the model.\n        \n    Inputs: \n        threshold: any features with correlations greater than this value are removed\n    \n    Output: \n        dataframe that contains only the non-highly-collinear features\n    '''\n    \n    # Dont want to remove correlations between loss\n    y = x['loss']\n    x = x.drop(columns = ['loss'])\n    \n    # Calculate the correlation matrix\n    corr_matrix = x.corr()\n    iters = range(len(corr_matrix.columns) - 1)\n    drop_cols = []\n\n    # Iterate through the correlation matrix and compare correlations\n    for i in iters:\n        for j in range(i):\n            item = corr_matrix.iloc[j:(j+1), (i+1):(i+2)]\n            col = item.columns\n            row = item.index\n            val = abs(item.values)\n            \n            # If correlation exceeds the threshold\n            if val >= threshold:\n                # Print the correlated features and the correlation value\n                # print(col.values[0], \"|\", row.values[0], \"|\", round(val[0][0], 2))\n                drop_cols.append(col.values[0])\n\n    # Drop one of each pair of correlated columns\n    drops = set(drop_cols)\n    x = x.drop(columns = drops)\n    \n    # Add the score back in to the data\n    x['loss'] = y\n               \n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72a9179d42f2c3e287655146b97b7a11fcd005ed"},"cell_type":"code","source":"# Remove the collinear features above a specified correlation coefficient\ndata = remove_collinear_features(data, 0.6);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0714bb6b8bfe04988becfd704daf41edf9b1043a"},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e7ff6c82948ad99aee1b8c5025d4508fca9426f6","scrolled":true},"cell_type":"code","source":"# # # Split Into Training and Testing Sets\n\n# Separate out the features and targets\nfeatures = data.drop(columns='loss')\ntargets = pd.DataFrame(data['loss'])\n\n# Split into 80% training and 20% testing set\nX_train, X_test, y_train, y_test = train_test_split(features, targets, test_size = 0.2, random_state = 42)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e4416f1bb6e26b561922dae8c81670d70cc974c"},"cell_type":"code","source":"# # Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eeb1b6ca509da08d113656d439dcdaa6eb4b24bf"},"cell_type":"code","source":"# Convert y to one-dimensional array (vector)\ny_train = np.array(y_train).reshape((-1, ))\ny_test = np.array(y_test).reshape((-1, ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"62df1f869bdcb157c0f242247ebaa2b61e3d227a"},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"354fe7c290ae7d708590aeede77a2e1f4ba10aa5"},"cell_type":"code","source":"X_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59165d3c0c09f25fa49a8b70e13e057a75ebd662"},"cell_type":"code","source":"# # # Models to Evaluate\n\n# We will compare five different machine learning Cassification models:\n\n# 1 - Logistic Regression\n# 2 - K-Nearest Neighbors Classification\n# 3 - Suport Vector Machine\n# 4 - Naive Bayes\n# 5 - Random Forest Classification\n\n# Function to calculate mean absolute error\ndef cross_val(X_train, y_train, model):\n    # Applying k-Fold Cross Validation\n    from sklearn.model_selection import cross_val_score\n    accuracies = cross_val_score(estimator = model, X = X_train, y = y_train, cv = 5)\n    return accuracies.mean()\n\n# Takes in a model, trains the model, and evaluates the model on the test set\ndef fit_and_evaluate(model):\n    \n    # Train the model\n    model.fit(X_train, y_train)\n    \n    # Make predictions and evalute\n    model_pred = model.predict(X_test)\n    model_cross = cross_val(X_train, y_train, model)\n    \n    # Return the performance metric\n    return model_cross","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5d839ce0829f4d6205b69767b7fa84b0d547744"},"cell_type":"code","source":"# # Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nnaive = GaussianNB()\nnaive_cross = fit_and_evaluate(naive)\n\nprint('Naive Bayes Performance on the test set: Cross Validation Score = %0.4f' % naive_cross)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e90ced9e87d28f2532dd5f9856e41d59e52ed11b"},"cell_type":"code","source":"# # Random Forest Classification\nfrom sklearn.ensemble import RandomForestClassifier\nrandom = RandomForestClassifier(n_estimators = 10, criterion = 'entropy')\nrandom_cross = fit_and_evaluate(random)\n\nprint('Random Forest Performance on the test set: Cross Validation Score = %0.4f' % random_cross)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"687f4e0ee16b1605f62da51937433c6b1fb1da19"},"cell_type":"code","source":"# # Gradiente Boosting Classification\nfrom xgboost import XGBClassifier\ngb = XGBClassifier()\ngb_cross = fit_and_evaluate(gb)\n\nprint('Gradiente Boosting Classification Performance on the test set: Cross Validation Score = %0.4f' % gb_cross)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"64b9e4bc8bd315eccbb57e2b4fdc69497bc6414b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"535f8320cd4758fd3c95c23eebca4d64da87f192"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da59ddb65c4111fcf40bad22703b180ad40816ee"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}